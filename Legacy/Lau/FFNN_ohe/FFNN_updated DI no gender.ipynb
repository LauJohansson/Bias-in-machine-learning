{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python\n", "# coding: utf-8"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[1]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ATH=\"/content/drive/My Drive/Kandidat speciale/500 - Notebooks/models/all races_CV50/\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["## AIR ### "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["AIR=True"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["file_name=\"Fall_count_clusterOHE_std.csv\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["full_file_path=\"/restricted/s164512/G2020-57-Aalborg-bias/data_air/\"+file_name"]}, {"cell_type": "markdown", "metadata": {}, "source": ["itel_mitigation=\"original\"<br>\n", "itel_mitigation=\"DroppingD\"<br>\n", "itel_mitigation=\"Gender Swap\"<br>\n", "itel_mitigation=\"DI remove\"<br>\n", "itel_mitigation=\"LFR\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["titel_mitigation=\"DI remove no gender\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["PATH_orig=\"/restricted/s164512/G2020-57-Aalborg-bias/lau/FFNN_ohe/models/\"+titel_mitigation+\"/\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dropping_D=True\n", "gender_swap=False\n", "DI_remove=True\n", "LFR_mitigation=False #S\u00c3\u00a6t droppingD=True, men ikke fjern den fra X"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_col_name=\"Fall\"\n", "X_col_names=[\n", "#'Gender',\n", "'BirthYear',\n", "'LoanPeriod',\n", "'NumberAts',\n", "'Ats_Polstring',\n", "'Ats_Mobilitystokke',\n", "'Ats_Belysning',\n", "'Ats_Underlag',\n", "'Ats_Toiletforh\u00c3\u00b8jereStativ',\n", "'Ats_Signalgivere',\n", "'Ats_EldrevneK\u00c3\u00b8restole',\n", "'Ats_Forst\u00c3\u00b8rrelsesglas',\n", "'Ats_N\u00c3\u00b8dalarmsystemer',\n", "'Ats_MobilePersonl\u00c3\u00b8ftere',\n", "'Ats_TrappelifteMedPlatforme',\n", "'Ats_Badekarsbr\u00c3\u00a6tter',\n", "'Ats_Albuestokke',\n", "'Ats_MaterialerOgRedskaberTilAfm\u00c3\u00a6rkning',\n", "'Ats_Rygl\u00c3\u00a6n',\n", "#'Ats_0',\n", "'Ats_Ganghj\u00c3\u00a6lpemidlerSt\u00c3\u00b8tteTilbeh\u00c3\u00b8r',\n", "'Ats_St\u00c3\u00b8tteb\u00c3\u00b8jler',\n", "'Ats_Lejringspuder',\n", "'Ats_Str\u00c3\u00b8mpep\u00c3\u00a5tagere',\n", "'Ats_D\u00c3\u00b8rtrin',\n", "'Ats_Spil',\n", "'Ats_BordeP\u00c3\u00a5Stole',\n", "'Ats_Drejeskiver',\n", "'Ats_Toiletstole',\n", "'Ats_L\u00c3\u00b8ftereStation\u00c3\u00a6re',\n", "'Ats_Madm\u00c3\u00a5lingshj\u00c3\u00a6lpemidler',\n", "'Ats_Fodbeskyttelse',\n", "'Ats_St\u00c3\u00a5l\u00c3\u00b8ftere',\n", "'Ats_Stole',\n", "'Ats_Sengeborde',\n", "'Ats_Toiletter',\n", "'Ats_Toiletforh\u00c3\u00b8jereFaste',\n", "'Ats_P\u00c3\u00a5kl\u00c3\u00a6dning',\n", "'Ats_Brusere',\n", "'Ats_V\u00c3\u00a6vsskadeLiggende',\n", "'Ats_D\u00c3\u00b8r\u00c3\u00a5bnere',\n", "'Ats_ServeringAfMad',\n", "'Ats_TrappelifteMedS\u00c3\u00a6der',\n", "'Ats_S\u00c3\u00a6derTilMotork\u00c3\u00b8ret\u00c3\u00b8jer',\n", "'Ats_K\u00c3\u00b8restoleManuelleHj\u00c3\u00a6lper',\n", "'Ats_Gangbukke',\n", "'Ats_Rollatorer',\n", "'Ats_Tryks\u00c3\u00a5rsforebyggendeSidde',\n", "'Ats_Fastnettelefoner',\n", "'Ats_B\u00c3\u00a6kkener',\n", "'Ats_Vendehj\u00c3\u00a6lpemidler',\n", "'Ats_Sanseintegration',\n", "'Ats_K\u00c3\u00b8restolsbeskyttere',\n", "'Ats_Arbejdsstole',\n", "'Ats_L\u00c3\u00b8ftesejl',\n", "'Ats_K\u00c3\u00b8restoleForbr\u00c3\u00a6ndingsmotor',\n", "'Ats_L\u00c3\u00b8ftestropper',\n", "'Ats_Stiger',\n", "'Ats_TransportTrapper',\n", "'Ats_DrivaggregaterK\u00c3\u00b8restole',\n", "'Ats_Emballage\u00c3\u00a5bnere',\n", "'Ats_Toiletforh\u00c3\u00b8jereL\u00c3\u00b8se',\n", "'Ats_H\u00c3\u00a5rvask',\n", "'Ats_Personl\u00c3\u00b8ftereStation\u00c3\u00a6re',\n", "'Ats_Madrasser',\n", "'Ats_Vindues\u00c3\u00a5bnere',\n", "'Ats_L\u00c3\u00a6sestativer',\n", "'Ats_K\u00c3\u00b8restoleManuelleDrivringe',\n", "'Ats_S\u00c3\u00a6depuder',\n", "'Ats_UdstyrCykler',\n", "'Ats_Karkludsvridere',\n", "'Ats_Vaskeklude',\n", "'Ats_Sengeudstyr',\n", "'Ats_Madlavningshj\u00c3\u00a6lpemidler',\n", "'Ats_Skohorn',\n", "'Ats_Gribet\u00c3\u00a6ngerManuelle',\n", "'Ats_Hvilestole',\n", "'Ats_EldrevneK\u00c3\u00b8restoleStyring',\n", "'Ats_B\u00c3\u00a6rehj\u00c3\u00a6lpemidlerTilK\u00c3\u00b8restole',\n", "'Ats_L\u00c3\u00b8ftegalgerSeng',\n", "'Ats_H\u00c3\u00b8reforst\u00c3\u00a6rkere',\n", "'Ats_Kalendere',\n", "'Ats_Stokke',\n", "'Ats_L\u00c3\u00b8ftegalger',\n", "'Ats_Ure',\n", "'Ats_St\u00c3\u00b8ttegrebFlytbare',\n", "'Ats_Forflytningsplatforme',\n", "'Ats_RamperFaste',\n", "'Ats_Rygehj\u00c3\u00a6lpemidler',\n", "'Ats_Personv\u00c3\u00a6gte',\n", "'Ats_Man\u00c3\u00b8vreringshj\u00c3\u00a6lpemidler',\n", "'Ats_Overt\u00c3\u00b8j',\n", "'Ats_Lydoptagelse',\n", "'Ats_Gangborde',\n", "'Ats_St\u00c3\u00a5st\u00c3\u00b8ttestole',\n", "'Ats_RamperMobile',\n", "'Ats_B\u00c3\u00a6rehj\u00c3\u00a6lpemidler',\n", "'Ats_Badekarss\u00c3\u00a6der',\n", "'Ats_Siddemodulsystemer',\n", "'Ats_Videosystemer',\n", "'Ats_Siddepuder',\n", "'Ats_Sengeheste',\n", "'Ats_Stolerygge',\n", "'Ats_Rulleborde',\n", "'Ats_Sengeforl\u00c3\u00a6ngere',\n", "'Ats_Madningsudstyr',\n", "'Ats_Brusestole',\n", "'Ats_Flerpunktsstokke',\n", "'Ats_SengebundeMedMotor',\n", "'Ats_Cykler',\n", "'Ats_CykelenhederK\u00c3\u00b8restole',\n", "'Ats_Stokkeholdere',\n", "'Ats_Toiletarmst\u00c3\u00b8tter',\n", "'Ats_Coxitstole',\n", "'Ats_Toilets\u00c3\u00a6der',\n", "'Ats_Rebstiger',\n", "'Ats_Forh\u00c3\u00b8jerklodser',\n", "'Cluster_0',\n", "'Cluster_1',\n", "'Cluster_2',\n", "'Cluster_3',\n", "'Cluster_4',\n", "'Cluster_5',\n", "'Cluster_6',\n", "'Cluster_7',\n", "'Cluster_8',\n", "'Cluster_9',\n", "'Cluster_10',\n", "'Cluster_11',\n", "'Cluster_12',\n", "'Cluster_13',\n", "'Cluster_14',\n", "'Cluster_15',\n", "'Cluster_16',\n", "'Cluster_17',\n", "'Cluster_18',\n", "'Cluster_19']\n", "#X_col_names = [col for col in X_col_names if col not in leave_out ]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["procted_col_name=\"Gender\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### COMPASS ####"]}, {"cell_type": "markdown", "metadata": {}, "source": ["IR=False"]}, {"cell_type": "markdown", "metadata": {}, "source": ["itel_mitigation=\"testCOMPAS\"<br>\n", "ATH_orig=\"/restricted/s164512/G2020-57-Aalborg-bias/lau/FFNN/models/\"+titel_mitigation+\"/\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["ull_file_path = 'https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_col_name=\"is_recid\"<br>\n", "_col_names=['remember_index','sex','age','race', 'juv_fel_count','juv_misd_count','juv_other_count','priors_count',\"c_charge_desc\",\"c_charge_degree\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["rocted_col_name=\"race\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[2]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def LFR_custom(df_train,y_train,lfr=None):\n", "    from aif360.algorithms.preprocessing import LFR\n", "    from aif360.datasets import BinaryLabelDataset\n", "    \n", "    df_train=pd.concat([df_train,y_train],axis=1)\n", "    \n", "    X_col_names_f=['Gender', 'BirthYear', 'LoanPeriod', 'NumberAts']\n", "    df2_all=df_train.drop(columns=X_col_names_f).copy() #Gemmer alle kolonner, undtagen numerical og gender\n", "    df2=df_train[X_col_names_f+[\"Fall\"]].copy() #Gem kun numerical features\n", "    df2_gender=df_train[\"Gender\"].copy() #Gemmer bare gender\n", "    \n", "    \n", "    #Create the binarylabeldataset\n", "    df_BLD = BinaryLabelDataset(favorable_label='1',\n", "                                unfavorable_label='0',\n", "                                df=df2,\n", "                                label_names=['Fall'],\n", "                                protected_attribute_names=[\"Gender\"],\n", "                                unprivileged_protected_attributes=['0'])\n", "    #Define the DI remover\n", "    if lfr is None:\n", "        lfr = LFR(privileged_groups=[{\"Gender\": 1}], \n", "                                    unprivileged_groups=[{\"Gender\": 0}])\n", "        rp_df = lfr.fit_transform(df_BLD)\n", "    else:\n", "        rp_df = lfr.transform(df_BLD)\n", "        \n\n", "    #Save the columnnames\n", "    all_col_names=df_BLD.feature_names+df_BLD.label_names\n", "        \n", "        \n", "    \n", "    #Save repaired data as pandas DF\n", "    rp_df_pd = pd.DataFrame(np.hstack([rp_df.features,rp_df.labels]),columns=all_col_names) \n", "    \n", "    #Somehow gender is also transformed! So we drop it! DETTE SKAL VI NOK LIGE HOLDE \u00c3\u02dcJE MED\n", "    ###OBS!#####\n", "    rp_df_pd = rp_df_pd.drop(columns=[\"Gender\"])\n", "    #rp_df_pd = pd.concat([rp_df_pd,df2_gender],axis=1)\n\n", "    ##########\n", "    \n", "    \n", "    #Concatenate the non-numerical columns\n", "    transformed_data = pd.concat ([rp_df_pd,df2_all], axis=1)\n", "    \n", "    \n", "    transformed_data=transformed_data.drop(columns=[\"Fall\"])\n", "    \n", "    return transformed_data,lfr"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[3]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def DI_remove_custom(df_train,RP_level=1.0,drop_d=False,y_train=None):\n", "    from aif360.algorithms.preprocessing import DisparateImpactRemover\n", "    from aif360.datasets import BinaryLabelDataset\n", "    \n", "    if drop_d:\n", "        df_train=pd.concat([df_train,y_train],axis=1)\n", "    \n", "    X_col_names_f=['Gender', 'BirthYear', 'LoanPeriod', 'NumberAts']\n", "    df2_all=df_train.drop(columns=X_col_names_f).copy() #Gemmer alle kolonner, undtagen numerical og gender\n", "    df2=df_train[X_col_names_f].copy() #Gem kun numerical features\n", "    \n", "    df2[\"dummy\"]=1 # this is a dummy variable, since DI remover dont use y. \n", "    \n", "    #Create the binarylabeldataset\n", "    df_BLD = BinaryLabelDataset(favorable_label='1',\n", "                                unfavorable_label='0',\n", "                                df=df2,\n", "                                label_names=['dummy'],\n", "                                protected_attribute_names=[\"Gender\"],\n", "                                unprivileged_protected_attributes=['0'])\n", "    #Define the DI remover\n", "    di = DisparateImpactRemover(repair_level=RP_level)\n", "    #Save the columnnames\n", "    all_col_names=df_BLD.feature_names+df_BLD.label_names\n", "    #Reparing the data\n", "    rp_df = di.fit_transform(df_BLD)  \n", "    #Save repaired data as pandas DF\n", "    rp_df_pd = pd.DataFrame(np.hstack([rp_df.features,rp_df.labels]),columns=all_col_names) \n", "    #Concatenate the non-numerical columns\n", "    transformed_data = pd.concat ([rp_df_pd,df2_all], axis=1)\n", "    \n", "    \n", "    transformed_data_train=transformed_data.drop(columns=[\"dummy\"])\n", "    \n", "    if drop_d:\n", "        transformed_data_train=transformed_data_train.drop(columns=[\"Gender\"])\n", "    \n", "    return transformed_data_train"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[4]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_nodes=500"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["batch_size=40\n", "epochs=400\n", "p_drop=0.4"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["optim_type=\"Adam\" #SGD\n", "lr=0.001 #0.001 er godt\n", "wd=0.05       "]}, {"cell_type": "markdown", "metadata": {}, "source": [".01 er godt til AIR ny!!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[5]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from tqdm.notebook import tqdm\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "#import torchvision.datasets as datasets\n", "from torch.utils.data import DataLoader\n", "#import torchvision.transforms as transforms\n", "import matplotlib.pyplot as plt\n", "import torch.optim as optim\n", "from IPython.display import clear_output"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd \n", "import seaborn as sns"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "plt.style.use('seaborn')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import plot_confusion_matrix\n", "from sklearn.metrics import confusion_matrix\n", "import matplotlib.patches as mpatches\n", "from matplotlib.patches import Patch\n", "from matplotlib.lines import Line2D"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from sklearn.model_selection import train_test_split\n", "from sklearn import preprocessing"]}, {"cell_type": "markdown", "metadata": {}, "source": ["rom google.colab import drive"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import KFold"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from datetime import datetime"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pytz\n", "import random"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "from sklearn.model_selection import StratifiedKFold"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[6]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot([0,0])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[7]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from utils_Copy import *"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[8]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def loss_fn(target,predictions):\n", "    criterion = nn.BCELoss()\n", "    loss_out = criterion(predictions, target)\n", "    return loss_out"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[9]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def accuracy(true,pred):\n", "    acc = (true.float().round() == pred.float().round()).float().detach().cpu().numpy()\n", "    return float(100 * acc.sum() / len(acc))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_test():\n", "    avg_loss_ts = 0\n", "    avg_acc_ts=0\n", "    model.eval()  # train mode\n", "    for X_batch, Y_batch in data_ts:\n", "        X_batch = X_batch.to(device)\n", "        Y_batch = Y_batch.to(device)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        # forward\n", "        Y_pred = model(X_batch.float()) \n", "        loss = loss_fn(Y_batch.float(), Y_pred.squeeze()) \n\n", "        # calculate metrics to show the user\n", "        avg_loss_ts += loss / len(data_ts)\n", "        avg_acc_ts+=accuracy(Y_batch,Y_pred.squeeze()) / len(data_ts)\n", "    #toc = time()\n", "    return avg_loss_ts, avg_acc_ts"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_all_time_low(all_time,new_val):\n", "    if all_time>new_val:\n", "        return new_val\n", "    else:\n", "        return all_time\n", "    \n", "def get_all_time_high(all_time,new_val):\n", "    if all_time<new_val:\n", "        return new_val\n", "    else:\n", "        return all_time"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[10]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Network(nn.Module):\n", "    def __init__(self):\n", "        super(Network, self).__init__()\n", "        self.fully_connected1 = nn.Sequential(\n", "            nn.Linear(n_feat,n_nodes),\n", "            nn.ReLU(),\n", "            nn.BatchNorm1d(n_nodes),\n", "            nn.Dropout(p_drop)\n", "            )\n", "        self.fully_connectednew = nn.Sequential(\n", "            nn.Linear(n_nodes,n_nodes),\n", "            nn.ReLU(),\n", "            nn.BatchNorm1d(n_nodes),\n", "            nn.Dropout(p_drop)\n", "            )\n", "        self.fully_connectednew1 = nn.Sequential(\n", "            nn.Linear(n_nodes,n_nodes),\n", "            nn.ReLU(),\n", "            nn.BatchNorm1d(n_nodes),\n", "            nn.Dropout(p_drop)\n", "            )\n", "        self.fully_connectednew2 = nn.Sequential(\n", "            nn.Linear(n_nodes,n_nodes),\n", "            nn.ReLU(),\n", "            nn.BatchNorm1d(n_nodes),\n", "            nn.Dropout(p_drop)\n", "            )\n", "        self.fully_connectednew3 = nn.Sequential(\n", "            nn.Linear(n_nodes,n_nodes),\n", "            nn.ReLU(),\n", "            nn.BatchNorm1d(n_nodes),\n", "            nn.Dropout(p_drop)\n", "            )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        self.fully_connected2 = nn.Sequential(\n", "            nn.Linear(n_nodes,output_dim),\n", "            #nn.Softmax(dim = 1)\n", "            nn.Sigmoid()\n", "            )\n", "    def forward(self, x):\n", "      #reshaping x so it becomes flat, except for the first dimension (which is the minibatch)\n", "        #x = x.view(x.size(0),-1)\n", "        x = self.fully_connected1(x)\n", "        x = self.fully_connectednew(x)\n", "        x = self.fully_connectednew1(x)\n", "        x = self.fully_connectednew2(x)\n", "        x = self.fully_connectednew3(x)\n", "        x = self.fully_connected2(x)\n", "        return x"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[11]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def custom_create_indexes(df,n,seed,strat=False,y_col=None):\n", "    list_of_index=[]\n", "    \n", "    \n", "    \n", "    if strat==False:\n", "        kf=KFold(n_splits=n, random_state=seed, shuffle=True)\n", "        \n", "        \n", "        for train_index, test_index in kf.split(df):\n", "            list_of_index.append(test_index)\n", "        tr_val_ts_indexes=[\n", "        #[[train],[validate],[test]]\n", "        [[*list_of_index[0],*list_of_index[1],*list_of_index[2]],list_of_index[3],list_of_index[4]],\n", "        [[*list_of_index[4],*list_of_index[0],*list_of_index[1]],list_of_index[2],list_of_index[3]],\n", "        [[*list_of_index[3],*list_of_index[4],*list_of_index[0]],list_of_index[1],list_of_index[2]],\n", "        [[*list_of_index[2],*list_of_index[3],*list_of_index[4]],list_of_index[0],list_of_index[1]],\n", "        [[*list_of_index[1],*list_of_index[2],*list_of_index[3]],list_of_index[4],list_of_index[0]],\n", "        ]\n", "    \n", "    \n", "    \n", "    \n", "    else:\n", "        kf = StratifiedKFold(n_splits=n, random_state=seed, shuffle=True)\n", "    \n", "    \n", "        for train_index, test_index in kf.split(df,df[y_col]):\n", "            list_of_index.append(test_index)\n", "        tr_val_ts_indexes=[\n", "        #[[train],[validate],[test]]\n", "        [[*list_of_index[0],*list_of_index[1],*list_of_index[2]],list_of_index[3],list_of_index[4]],\n", "        [[*list_of_index[4],*list_of_index[0],*list_of_index[1]],list_of_index[2],list_of_index[3]],\n", "        [[*list_of_index[3],*list_of_index[4],*list_of_index[0]],list_of_index[1],list_of_index[2]],\n", "        [[*list_of_index[2],*list_of_index[3],*list_of_index[4]],list_of_index[0],list_of_index[1]],\n", "        [[*list_of_index[1],*list_of_index[2],*list_of_index[3]],list_of_index[4],list_of_index[0]],\n", "        ]\n", "    \n", "    return tr_val_ts_indexes"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[12]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["modelcounter=0\n", "for custom_seed in range(1,11):\n", "    \n", "    torch.manual_seed(custom_seed)\n", "    random.seed(custom_seed)\n", "    np.random.seed(custom_seed)\n", "    \n", "    df2 = pd.read_csv(full_file_path)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["   \n", "    X=df2[X_col_names]\n", "    if dropping_D==True:\n", "        y=df2[[y_col_name,procted_col_name]]\n", "    else:\n", "        y=df2[[y_col_name]]\n\n", "    #https://stackoverflow.com/questions/11587782/creating-dummy-variables-in-pandas-for-python\n", "    if AIR==False:\n", "        just_dummies=pd.get_dummies(X[['sex',\"race\",\"c_charge_desc\",\"c_charge_degree\"]])\n", "        X = pd.concat([X, just_dummies], axis=1) \n", "        X=X.drop(['sex',\"race\",\"c_charge_desc\",\"c_charge_degree\"] ,axis=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    \n", "    tr_val_ts_indexes= custom_create_indexes(X,5,custom_seed)\n", "    #tr_val_ts_indexes= custom_create_indexes(df2,5,custom_seed,True,y_col_name) #stratify\n\n", "    #i=0\n", "    for mini_loop in range(len(tr_val_ts_indexes)):\n", "        print(\"Running overall number \"+str(modelcounter))\n", "         \n", "        \n", "        X_train_pd, y_train_pd = X.iloc[tr_val_ts_indexes[mini_loop][0]], y.iloc[tr_val_ts_indexes[mini_loop][0]]\n", "        X_val_pd, y_val_pd = X.iloc[tr_val_ts_indexes[mini_loop][1]], y.iloc[tr_val_ts_indexes[mini_loop][1]]\n", "        X_test_pd, y_test_pd = X.iloc[tr_val_ts_indexes[mini_loop][2]], y.iloc[tr_val_ts_indexes[mini_loop][2]]\n", "        \n", "        \n", "        seedName=\"model\"+str(modelcounter)\n", "        PATH=PATH_orig+seedName+\"/\"\n", "        print(PATH)\n\n", "        #Make dir to files\n", "        if not os.path.exists(PATH):\n", "            os.makedirs(PATH)\n", "            print(\"Created new path!: \",PATH)\n", "        \n", "        \n", "        if gender_swap==True:\n", "            X_train_pd_copy=X_train_pd.copy()\n", "            y_train_pd_copy=y_train_pd.copy()\n", "            \n", "            X_train_pd_copy[\"Gender\"]=(X_train_pd_copy[\"Gender\"]-1)*(-1)\n", "            \n", "            X_train_pd=pd.concat([X_train_pd,X_train_pd_copy])\n", "            \n", "            y_train_pd=pd.concat([y_train_pd,y_train_pd_copy])\n", "            \n", "            \n", "            \n", "            \n", "        if DI_remove==True:\n", "            X_train_pd=DI_remove_custom(X_train_pd.reset_index(drop=True),drop_d=dropping_D,y_train=y_train_pd.reset_index(drop=True))\n", "            X_val_pd=DI_remove_custom(X_val_pd.reset_index(drop=True),drop_d=dropping_D,y_train=y_val_pd.reset_index(drop=True))\n", "            X_test_pd=DI_remove_custom(X_test_pd.reset_index(drop=True),drop_d=dropping_D,y_train=y_test_pd.reset_index(drop=True))\n", "            \n", "            y_train_pd=y_train_pd.reset_index(drop=True)\n", "            \n", "            y_val_pd=y_val_pd.reset_index(drop=True)\n", "            \n", "            y_test_pd=y_test_pd.reset_index(drop=True)\n", "            \n", "            \n", "        if LFR_mitigation==True:\n", "            \n", "            \n", "            X_train_pd,lfr=LFR_custom(X_train_pd.reset_index(drop=True),\n", "                                  y_train_pd.reset_index(drop=True).drop(columns=[\"Gender\"]),\n", "                                 lfr=None\n", "                                 )\n", "            X_val_pd,lfr=LFR_custom(X_val_pd.reset_index(drop=True),\n", "                                  y_val_pd.reset_index(drop=True).drop(columns=[\"Gender\"]),\n", "                                 lfr=lfr\n", "                                 )\n", "            X_test_pd,lfr=LFR_custom(X_test_pd.reset_index(drop=True),\n", "                                  y_test_pd.reset_index(drop=True).drop(columns=[\"Gender\"]),\n", "                                 lfr=lfr\n", "                                 )\n", "            \n", "            y_train_pd=y_train_pd.reset_index(drop=True)\n", "            \n", "            y_val_pd=y_val_pd.reset_index(drop=True)\n", "            \n", "            y_test_pd=y_test_pd.reset_index(drop=True)\n", "            \n", "            \n", "            \n", "        X_train, y_train = X_train_pd, y_train_pd\n", "        X_val, y_val = X_val_pd, y_val_pd\n", "        X_test, y_test = X_test_pd, y_test_pd"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        #Save as numpy array for the DATALOADER (PyTorch)\n", "        \n", "        if LFR_mitigation==True:\n", "            temp_col_name_LFR=[name for name in X_col_names if name not in [\"Gender\"]]\n", "            X_train=np.array(X_train[temp_col_name_LFR])\n", "            y_train=np.array(y_train[y_col_name])\n", "            X_val=np.array(X_val[temp_col_name_LFR])\n", "            y_val=np.array(y_val[y_col_name])\n", "            X_test=np.array(X_test[temp_col_name_LFR])\n", "            y_test=np.array(y_test[y_col_name])\n", "        \n", "        \n", "        else:\n", "            \n", "            X_train=np.array(X_train[X_col_names])\n", "            y_train=np.array(y_train[y_col_name])\n", "            X_val=np.array(X_val[X_col_names])\n", "            y_val=np.array(y_val[y_col_name])\n", "            X_test=np.array(X_test[X_col_names])\n", "            y_test=np.array(y_test[y_col_name])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["       # print(\"X_train shape: {}\".format(X_train.shape))\n", "       # print(\"y_train shape: {}\".format(y_train.shape))\n\n", "        #print(\"X_val shape: {}\".format(X_val.shape))\n", "        #print(\"y_val shape: {}\".format(y_val.shape))\n\n", "        #print(\"X_test shape: {}\".format(X_test.shape))\n", "        #print(\"y_test shape: {}\".format(y_test.shape))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        #n_feat=X_train.shape[1]\n", "        \n", "        if LFR_mitigation==True:\n", "            n_feat=len(X_col_names)-1#minus gender\n", "        \n", "        else:\n", "            n_feat=len(X_col_names)\n", "        \n", "        \n", "        output_dim=1 #binary"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        data_tr = DataLoader(list(zip(X_train, y_train)), batch_size=batch_size, shuffle=False)\n", "        data_val = DataLoader(list(zip(X_val, y_val)), batch_size=batch_size, shuffle=False)\n", "        data_ts = DataLoader(list(zip(X_test, y_test)), batch_size=batch_size, shuffle=False)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n", "        #device=\"cpu\"\n", "        print(device)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        model = Network().to(device)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        opt=optim.Adam(model.parameters(),lr=lr, weight_decay = wd)\n", "        \n", "        epochnumber = []\n", "        all_train_losses = []\n", "        all_val_losses = []\n", "        all_ts_losses = []\n", "        all_train_acc=[]\n", "        all_val_acc=[]\n", "        all_ts_acc=[]\n", "        all_time_low_train_loss=1000\n", "        all_time_low_val_loss=1000\n", "        all_time_high_train_acc=0\n", "        all_time_high_val_acc=0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        for epoch in range(epochs):\n", "            if (epoch)%20==0:\n", "                print('* Epoch %d/%d' % (epoch+1, epochs))\n", "            epochnumber.append(epoch)\n", "            avg_loss_train = 0\n", "            avg_acc=0\n", "            model.train()  # train mode\n", "            for X_batch, Y_batch in data_tr:\n", "                X_batch = X_batch.to(device)\n", "                Y_batch = Y_batch.to(device)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["                # set parameter gradients to zero\n", "                opt.zero_grad()\n", "                # forward\n", "                Y_pred = model(X_batch.float()) #oprdindeligt havde vi 3 lag (RGB), nu har vi kun 1 (greyscale) -> \n", "                loss = loss_fn(Y_batch.float(), Y_pred.squeeze())  # forward-pass\n", "                loss.backward()  # backward-pass\n", "                opt.step()  # update weights\n", "                # calculate metrics to show the user\n", "                avg_loss_train += loss / len(data_tr)\n", "                avg_acc+=accuracy(Y_batch,Y_pred.squeeze()) / len(data_tr)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["            all_time_low_train_loss=get_all_time_low(all_time_low_train_loss,avg_loss_train)\n", "            all_time_low_train_acc=get_all_time_high(all_time_high_train_acc,avg_acc)\n", "              #print(' - train loss: %f' % avg_loss_train)\n", "              #print(' - train acc: {} %'.format(round(avg_acc,2)))\n", "            all_train_losses.append(avg_loss_train)\n", "            all_train_acc.append(avg_acc)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["            with torch.no_grad():\n", "                avg_loss_val = 0\n", "                avg_acc_val=0\n", "                model.eval()  # eval mode\n", "                for X_batch, Y_batch in data_val:\n", "                    X_batch = X_batch.to(device)\n", "                    Y_batch = Y_batch.to(device)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["                    # forward\n", "                    Y_pred = model(X_batch.float()) \n", "                    loss = loss_fn(Y_batch.float(), Y_pred.squeeze())  # forward-pass\n", "                    # calculate metrics to show the user\n", "                    avg_loss_val += loss / len(data_val)\n", "                    avg_acc_val+=accuracy(Y_batch,Y_pred.squeeze()) / len(data_val)\n", "                #toc = time()\n", "                all_time_low_val_loss=get_all_time_low(all_time_low_val_loss,avg_loss_val)\n", "                all_time_low_val_acc=get_all_time_high(all_time_high_val_acc,avg_acc_val)\n", "                #print(' - val loss: %f' % avg_loss_val)\n", "                #print(' - val acc: {} %'.format(round(avg_acc_val,2)))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["                ########Save model####\n", "            if  epoch == 0 or avg_loss_val <= min(all_val_losses) :\n", "                torch.save(model.state_dict(), PATH+'_FFNN_model_local.pth')\n", "                print('####Saved model####')\n", "            all_val_losses.append(avg_loss_val)\n", "            all_val_acc.append(avg_acc_val)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["          ###PLOT########\n", "        if epoch==epochs-1:\n", "            #Save the last epoch\n", "            torch.save(model.state_dict(), PATH+'_FFNN_model_global.pth')\n\n", "            #take the best model (with lowest validation loss)\n", "            model.load_state_dict(torch.load(PATH+'_FFNN_model_local.pth'))\n", "            model.eval()\n", "            all_ts_losses=[get_test()[0]] * (epoch+1)\n", "            all_ts_acc=[get_test()[1]] * (epoch+1)\n", "            plt.figure(1)\n", "            plt.plot(epochnumber, all_train_losses, 'r', epochnumber, all_val_losses, 'b',epochnumber, all_ts_losses, '--')\n", "            plt.xlabel('Epochs'), plt.ylabel('Loss')\n", "            plt.legend(['Train Loss', 'Val Loss','Test loss'])\n", "            plt.savefig(PATH+'_loss.png')\n", "            plt.show()\n", "            plt.figure(2)\n", "            plt.plot(epochnumber, all_train_acc, 'black', epochnumber, all_val_acc, 'grey',epochnumber, all_ts_acc, '--')\n", "            plt.xlabel('Epochs'), plt.ylabel('Accuracy')\n", "            plt.legend(['Train acc', 'Val acc','Test acc'])\n", "            plt.savefig(PATH+'_acc.png')\n", "            plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["            metrics=pd.DataFrame({\"all_time_low_train_loss\":[all_time_low_train_loss.item()],\n", "                                  \"all_time_low_train_acc\":[all_time_low_train_acc],\n", "                              \"all_time_low_val_loss\":[all_time_low_val_loss.item()],\n", "                                  \"all_time_val_train_acc\":[all_time_low_val_acc],\n", "                              \"test_acc\":[all_ts_acc[0]],\n", "                              \"test_loss\":[all_ts_losses[0].item()]\n", "                                                })\n", "            metrics.to_csv( PATH+'_metrics.csv')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        for local_best in [0,1]:\n", "            #local_best=0\n", "            model1 = Network().to(device)\n", "            if local_best==1:\n", "                model1.load_state_dict(torch.load(PATH+'_FFNN_model_local.pth'))\n", "            else:\n", "                model1.load_state_dict(torch.load(PATH+'_FFNN_model_global.pth'))\n", "            model1.eval()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["            df_evaluate = X_test_pd.copy()\n", "            df_evaluate[y_col_name]=y_test_pd[y_col_name]\n", "            \n", "            if dropping_D==True:\n", "                df_evaluate[procted_col_name]=y_test_pd[procted_col_name]\n", "            \n", "            else:\n", "                df_evaluate[procted_col_name]=X_test_pd[procted_col_name]\n", "            if AIR==False:\n", "                cols= [col for col in list(df_evaluate.columns) if col not in [y_col_name,\"sex\",\"age_cat\",\"race\",\"c_charge_desc\",\"c_charge_degree\"]]\n", "            else:\n", "                if dropping_D==True:\n", "                    cols= [col for col in list(df_evaluate.columns) if col not in [y_col_name,procted_col_name]]\n", "                    \n", "                elif gender_swap==True:\n", "                    cols= [col for col in list(df_evaluate.columns) if col not in [y_col_name,\"Original\"]]\n", "                else:\n", "                    cols= [col for col in list(df_evaluate.columns) if col not in [y_col_name]]\n", "                    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["            X_numpy=np.array(df_evaluate[cols])\n", "            X_torch=torch.tensor(X_numpy)\n", "            y_pred = model1(X_torch.float().to(device))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["            list_of_output=[round(a.item(),0) for a in y_pred.detach().cpu()]\n", "            list_of_output_prob=[a.item() for a in y_pred.detach().cpu()]\n", "            df_evaluate[\"output\"]=list_of_output\n", "            df_evaluate[\"output_prob\"]=list_of_output_prob\n", "            df_evaluate[\"Model\"]=seedName"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["            ##SAVING THE TEST DATA\n", "            if local_best==1:\n", "                df_evaluate.to_csv(PATH+\"test_data_localmodel.csv\")\n", "            else:\n", "                df_evaluate.to_csv(PATH+\"test_data_globalmodel.csv\")\n", "            \n", "        modelcounter=modelcounter+1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Save all test data (and output)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[14]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for file_name in [\"localmodel\",\"globalmodel\"]:\n", "    first_time=True\n", "    \n", "    for j in range(modelcounter):\n", "        \n", "        if first_time==True:\n", "            test_data_all = pd.read_csv(PATH_orig+\"model\"+str(j)+\"/test_data_\"+file_name+\".csv\")\n", "            first_time=False\n", "        else:\n", "            test_subset=pd.read_csv(PATH_orig+\"model\"+str(j)+\"/test_data_\"+file_name+\".csv\")\n", "            test_data_all=pd.concat([test_data_all,test_subset],sort=False,axis=0)\n", "       \n", "       \n", "    test_data_all.to_csv(PATH_orig+\"all_test_data_\"+file_name+\".csv\")\n", "    print(f\"the shape of {file_name} is {test_data_all.shape}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": [" # GLOBAL ALL"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["olumn_names = [\"Group\", \"ML\", \"Measure\",\"Value\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["f_out = pd.DataFrame(columns = column_names)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["or i in range(50):"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  \n", "#    PATH_loop=PATH_orig+\"model\"+str(i)+\"/_all_stats_global.csv\"\n", "  \n", "#    data=pd.read_csv(PATH_loop)\n", "#    for group in [\"all\"]:\n", "#        for measure in ['FPR', 'FNR', 'ACC', 'F1', 'FDR', 'LRminus','LRplus', 'NPV', 'PPV', 'TNR', 'TPR','TP','TN','FN','FP']:\n", "#            value=float(data[data[procted_col_name]==group][measure])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["           df_out=df_out.append({'Group': group,\"ML\":\"FFNN\"+str(i),\"Measure\":measure,\"Value\":value}, ignore_index=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["f_out.to_csv(PATH_orig+\"/FFNN_metrics_crossvalidated_global_all.csv\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["lobal_all_bar=sns.barplot(data=df_out[df_out[\"Measure\"].isin([\"FPR\",\"FNR\",\"TPR\",\"TNR\"])],x=\"Group\", y=\"Value\", ci=95,hue=\"Measure\")<br>\n", "lobal_all_bar.set_title('Global all')<br>\n", "lobal_all_bar.get_figure().savefig(PATH_orig+\"/barplot_global_all.png\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# LOCAL ALL"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["olumn_names = [\"Group\", \"ML\", \"Measure\",\"Value\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["f_out = pd.DataFrame(columns = column_names)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["or i in range(50):<br>\n", "<br>\n", "   PATH_loop=PATH_orig+\"model\"+str(i)+\"/_all_stats_local.csv\"<br>\n", " <br>\n", "   data=pd.read_csv(PATH_loop)<br>\n", "   for group in [\"all\"]:<br>\n", "       for measure in ['FPR', 'FNR', 'ACC', 'F1', 'FDR', 'LRminus','LRplus', 'NPV', 'PPV', 'TNR', 'TPR','TP','TN','FN','FP',\"y_hat_mean\",\"y_target_mean\"]:<br>\n", "           value=float(data[data[procted_col_name]==group][measure])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["           df_out=df_out.append({'Group': group,\"ML\":\"FFNN\"+str(i),\"Measure\":measure,\"Value\":value}, ignore_index=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["f_out.to_csv(PATH_orig+\"FFNN_metrics_crossvalidated_local_all.csv\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ocal_all_bar=sns.barplot(data=df_out[df_out[\"Measure\"].isin([\"FPR\",\"FNR\",\"TPR\",\"TNR\"])],x=\"Group\", y=\"Value\", ci=95,hue=\"Measure\")<br>\n", "ocal_all_bar.set_title('Global all')<br>\n", "ocal_all_bar.get_figure().savefig(PATH_orig+\"/barplot_local_all.png\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Global protected"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["olumn_names = [\"Group\", \"ML\", \"Measure\",\"Value\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["f_out = pd.DataFrame(columns = column_names)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["or i in range(50):"]}, {"cell_type": "markdown", "metadata": {}, "source": ["   PATH_loop=PATH_orig+\"model\"+str(i)+\"/_\"+procted_col_name+\"_stats_global.csv\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  \n", "#    data=pd.read_csv(PATH_loop)\n", "#    for group in list(data[procted_col_name].unique()):\n", "#        for measure in ['FPR', 'FNR', 'ACC', 'F1', 'FDR', 'LRminus','LRplus', 'NPV', 'PPV', 'TNR', 'TPR','TP','TN','FN','FP',\"y_hat_mean\",\"y_target_mean\"]:\n", "#            value=float(data[data[procted_col_name]==group][measure])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [" #           df_out=df_out.append({'Group': group,\"ML\":\"FFNN\"+str(i),\"Measure\":measure,\"Value\":value}, ignore_index=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["f_out.to_csv(PATH_orig+\"FFNN_metrics_crossvalidated_global_\"+procted_col_name+\".csv\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["lobal_proc_bar=sns.barplot(data=df_out[df_out[\"Measure\"].isin([\"FPR\",\"FNR\",\"TPR\",\"TNR\"])],x=\"Group\", y=\"Value\", ci=95,hue=\"Measure\")<br>\n", "lobal_proc_bar.set_title('Global proctected: '+procted_col_name)<br>\n", "lobal_proc_bar.get_figure().savefig(PATH_orig+\"/barplot_global_proc.png\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Local protected"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["olumn_names = [\"Group\", \"ML\", \"Measure\",\"Value\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["f_out = pd.DataFrame(columns = column_names)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["or i in range(50):<br>\n", "   PATH_loop=PATH_orig+\"model\"+str(i)+\"/_\"+procted_col_name+\"_stats_local.csv\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  \n", "#    data=pd.read_csv(PATH_loop)\n", "#    for group in list(data[procted_col_name].unique()):\n", "#        for measure in ['FPR', 'FNR', 'ACC', 'F1', 'FDR', 'LRminus','LRplus', 'NPV', 'PPV', 'TNR', 'TPR','TP','TN','FN','FP',\"y_hat_mean\",\"y_target_mean\"]:\n", "#            value=float(data[data[procted_col_name]==group][measure])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["           df_out=df_out.append({'Group': group,\"ML\":\"FFNN\"+str(i),\"Measure\":measure,\"Value\":value}, ignore_index=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["f_out.to_csv(PATH_orig+\"FFNN_metrics_crossvalidated_local_\"+procted_col_name+\".csv\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ocal_proc_bar=sns.barplot(data=df_out[df_out[\"Measure\"].isin([\"FPR\",\"FNR\",\"TPR\",\"TNR\"])],x=\"Group\", y=\"Value\", ci=95,hue=\"Measure\")<br>\n", "ocal_proc_bar.set_title('Local protected: '+procted_col_name)<br>\n", "ocal_proc_bar.get_figure().savefig(PATH_orig+\"/barplot_local_proc.png\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "for file_name in [\"localmodel\",\"globalmodel\"]:<br>\n", "    <br>\n", "    for j in range(modelcounter):<br>\n", "    <br>\n", "        test_data_0 = pd.read_csv(PATH_orig+\"model0/test_data_\"+file_name+\".csv\")<br>\n", "        test_data_1 = pd.read_csv(PATH_orig+\"model1/test_data_\"+file_name+\".csv\")<br>\n", "        test_data_2 = pd.read_csv(PATH_orig+\"model2/test_data_\"+file_name+\".csv\")<br>\n", "        test_data_3 = pd.read_csv(PATH_orig+\"model3/test_data_\"+file_name+\".csv\")<br>\n", "        test_data_4 = pd.read_csv(PATH_orig+\"model4/test_data_\"+file_name+\".csv\")<br>\n", "        test_data_5 = pd.read_csv(PATH_orig+\"model5/test_data_\"+file_name+\".csv\")<br>\n", "        test_data_6 = pd.read_csv(PATH_orig+\"model6/test_data_\"+file_name+\".csv\")<br>\n", "        test_data_7 = pd.read_csv(PATH_orig+\"model7/test_data_\"+file_name+\".csv\")<br>\n", "        test_data_8 = pd.read_csv(PATH_orig+\"model8/test_data_\"+file_name+\".csv\")<br>\n", "        test_data_9 = pd.read_csv(PATH_orig+\"model9/test_data_\"+file_name+\".csv\")<br>\n", "        df2=    pd.concat([test_data_0,<br>\n", "                            test_data_1,<br>\n", "                            test_data_2,<br>\n", "                            test_data_3,<br>\n", "                            test_data_4,<br>\n", "                            test_data_5,<br>\n", "                            test_data_6,<br>\n", "                            test_data_7,<br>\n", "                            test_data_8,<br>\n", "                            test_data_9<br>\n", "                           ],sort=False,axis=0)<br>\n", "    df2.to_csv(PATH_orig+\"all_test_data_\"+file_name+\".csv\")<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}