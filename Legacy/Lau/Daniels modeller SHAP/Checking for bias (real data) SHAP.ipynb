{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python\n", "# coding: utf-8"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[1]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "from sklearn import preprocessing\n", "import math "]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[2]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["odelname=\"LR\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["modelname=\"SVM\"\n", "#modelname=\"RF\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[3]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_shape(modelname,model,xcolnames,X_train,X_test,directory,modelnr,plot_type=\"bar\"):\n", "    import shap\n", "   \n", "    if modelname.lower()==\"xgboost\" or modelname.lower()==\"rf\":\n", "        print(\"Using treeexplainer\")\n", "        shap_explainer = shap.TreeExplainer(model, data=X_train)\n", "        \n", "        if modelname.lower()==\"rf\":\n", "            shap_values = shap_explainer.shap_values(X_test,check_additivity=False)\n", "            shap_values=shap_values[1]\n", "        \n", "    elif modelname.lower()==\"ffnn\":\n", "        print(\"Using deepexplainer\")\n", "        shap_explainer = shap.DeepExplainer(model, data=X_train)\n", "        shap_values = shap_explainer.shap_values(X_test)\n", "    elif modelname.lower()==\"svm\":\n", "        print(\"Using kernelshap\")\n", "        shap_explainer = shap.KernelExplainer(model.predict_proba, data=shap.kmeans(X_train, 10))\n", "        shap_values = shap_explainer.shap_values(shap.sample(X_test, 100))\n", "        shap_values=shap_values[1]\n", "        \n", "        #https://slundberg.github.io/shap/notebooks/linear_explainer/Sentiment%20Analysis%20with%20Logistic%20Regression.html\n", "    elif modelname.lower()==\"lr\":\n", "        print(\"Using explin\")\n", "        background = shap.maskers.Independent(X_train, max_samples=100)\n", "        shap_explainer = shap.LinearExplainer(model=model, data=X_train,masker=background)\n", "        shap_values = shap_explainer.shap_values(X_test)\n", "        \n", "        \n", "    else:\n", "        raise Exception(\"Lau says: Sorry, cant find the model\")\n", "    \n", "    feature_names=xcolnames\n", "    \n", "    #Dette er Christians m\u00c3\u00a5de at hente values fra SHAP\n", "    importance_df  = pd.DataFrame()\n", "    importance_df['feature'] = feature_names\n", "    \n", "    importance_df['shap_values'] = np.around(np.array(shap_values)[:,:].mean(0), decimals=3)\n", "    importance_df['shap_values_abs'] = np.around(abs(np.array(shap_values)[:,:]).mean(0), decimals=3)\n", "    \n", "    \n", "    #if modelname.lower()==\"xgboost\":\n", "    #    importance_df['feat_imp'] = np.around(model.feature_importances_, decimals=3)\n", "    feat_importance_df_shap = importance_df.groupby('feature').mean().sort_values('shap_values',\n", "                                                                                   ascending=False)\n", "    feat_importance_df_shap = feat_importance_df_shap.reset_index()\n", "    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["   \n", "    feat_importance_df_shap.to_csv(directory+modelname+f\"best features model \"+str(modelnr)+\".csv\")\n", "    \n", "    \n", "    ##ALL VALUES###\n", "   \n", "    importance_df_all  = pd.DataFrame(shap_values,columns=feature_names)\n", "    importance_df_all.to_csv(directory+modelname+f\"best features model \"+str(modelnr)+\"_all.csv\")\n", "    \n", "    \n", "    \n", "    file_name_sum = \"shap_summary\"\n", "    file_name_exp = \"shap_row_0\"\n", "  \n", "    \n", "    plt.close()\n", "    shap.summary_plot(shap_values,\n", "                      X_test,\n", "                      feature_names=feature_names,\n", "                      plot_type=plot_type,\n", "                      show=False)\n", "    \n", "    plt.savefig(directory+\"/barplots/\"+modelname+\"shap_plot model\"+str(modelnr)+\".png\",\n", "                bbox_inches = \"tight\")\n", "    \n", "    if modelname.lower()!=\"svm\":\n", "        plt.close()\n", "        shap.summary_plot(shap_values,\n", "                          X_test,\n", "                          feature_names=feature_names,\n", "                          show=False)\n", "        plt.savefig(directory+\"/beeplots/\"+modelname+\"shap_plot beeswarm model\"+str(modelnr)+\".png\",\n", "                    bbox_inches = \"tight\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[4]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fall_data = pd.read_csv('/restricted/s164512/G2020-57-Aalborg-bias/Data_air/Fall_count_clusterOHE_std.csv')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Gender bias"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[5]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = fall_data.drop(columns=['Unnamed: 0','Fall']) # using all covariates in the dataset. ,'Ats_0'\n", "y = fall_data[['Gender','Fall']]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[6]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_col_names=list(X.columns)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[7]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn import svm, datasets\n", "from sklearn.metrics import plot_confusion_matrix\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.model_selection import KFold"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["modelcounter=0\n", "for i in range(1,11):\n", "    \n", "    kf=KFold(n_splits=5, random_state=i, shuffle=True)\n", "    \n", "    for train_index, test_index in kf.split(X):\n", "        print(\"Running \",modelname,\" nr \",modelcounter)\n", "        \n", "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n", "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n", "    \n", "    #### Flip fra den ene til den anden for at skifte mellem SVM, LR og RF ###\n", "    \n", "        if modelname==\"SVM\":\n", "            classifier = svm.SVC(kernel='rbf', C=1, random_state=0,class_weight='balanced',probability=True).fit(X_train, y_train['Fall'])\n", "        elif modelname==\"LR\":\n", "            classifier = LogisticRegression(max_iter=1000,class_weight='balanced').fit(X_train, y_train['Fall'])\n", "        elif modelname==\"RF\":\n", "            classifier = RandomForestClassifier(random_state=1).fit(X_train, y_train['Fall'])\n", "    \n", "    \n", "        #### Creating shap\n", "        create_shape(modelname,classifier,X_col_names,X_train,X_test,\"/restricted/s164512/G2020-57-Aalborg-bias/SHAP/\",modelcounter)\n", "        modelcounter=modelcounter+1\n", "    \n", "   "]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}