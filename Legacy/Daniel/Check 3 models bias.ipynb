{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python\n", "# coding: utf-8"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Imports"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[1]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from tqdm.notebook import tqdm\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "#import torchvision.datasets as datasets\n", "from torch.utils.data import DataLoader\n", "#import torchvision.transforms as transforms\n", "import matplotlib.pyplot as plt\n", "import torch.optim as optim\n", "from IPython.display import clear_output"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd \n", "import seaborn as sns"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "plt.style.use('seaborn')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import plot_confusion_matrix\n", "from sklearn.metrics import confusion_matrix\n", "import matplotlib.patches as mpatches\n", "from matplotlib.patches import Patch\n", "from matplotlib.lines import Line2D"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from sklearn.model_selection import train_test_split"]}, {"cell_type": "markdown", "metadata": {}, "source": ["rom google.colab import drive"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import KFold"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from datetime import datetime"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pytz\n", "import random"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# FILE PATH"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[4]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# USE ALL DATA<br>\n", "ile_name=\"fall.csv\"<br>\n", "ull_file_path=\"/restricted/s164512/G2020-57-Aalborg-bias/data_air/\"+file_name"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#USE SMALL TEST"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["PATH_orig=\"/restricted/s164512/G2020-57-Aalborg-bias/Daniel/\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["file_name=\"something.csv\"\n", "#file_name=\"all_test_data_globalmodel.csv\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["full_file_path=PATH_orig+file_name"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Specify the y, X and protected variable"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[5]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_col_name=\"Fall\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_col_names=['Gender', 'BirthYear', 'Cluster', 'LoanPeriod', 'NumberAts', '1Ats',\n", "       '2Ats', '3Ats', '4Ats', '5Ats', '6Ats', '7Ats', '8Ats', '9Ats', '10Ats',\n", "       '11Ats', '12Ats', '13Ats', '14Ats', '15Ats', '16Ats', '17Ats', '18Ats',\n", "       '19Ats', '20Ats', '21Ats', '22Ats', '23Ats', '24Ats', '25Ats', '26Ats',\n", "       '27Ats', '28Ats', '29Ats', '30Ats', '31Ats', '32Ats', '33Ats', '34Ats',\n", "       '35Ats', '36Ats', '37Ats', '38Ats', '39Ats', '40Ats', '41Ats', '42Ats',\n", "       '43Ats', '44Ats', '45Ats', '46Ats', '47Ats', '48Ats', '49Ats', '50Ats',]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["procted_col_name=\"Gender\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["output_col_name=\"output\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Read the data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[6]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df2 = pd.read_csv(full_file_path)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["est_data_0 = pd.read_csv(PATH_orig+\"model0/\"+file_name)<br>\n", "est_data_1 = pd.read_csv(PATH_orig+\"model1/\"+file_name)<br>\n", "est_data_2 = pd.read_csv(PATH_orig+\"model2/\"+file_name)<br>\n", "est_data_3 = pd.read_csv(PATH_orig+\"model3/\"+file_name)<br>\n", "est_data_4 = pd.read_csv(PATH_orig+\"model4/\"+file_name)<br>\n", "est_data_5 = pd.read_csv(PATH_orig+\"model5/\"+file_name)<br>\n", "est_data_6 = pd.read_csv(PATH_orig+\"model6/\"+file_name)<br>\n", "est_data_7 = pd.read_csv(PATH_orig+\"model7/\"+file_name)<br>\n", "est_data_8 = pd.read_csv(PATH_orig+\"model8/\"+file_name)<br>\n", "est_data_9 = pd.read_csv(PATH_orig+\"model9/\"+file_name)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["f2=    pd.concat([test_data_0,<br>\n", "                   test_data_1,<br>\n", "                   test_data_2,<br>\n", "                   test_data_3,<br>\n", "                   test_data_4,<br>\n", "                   test_data_5,<br>\n", "                   test_data_6,<br>\n", "                   test_data_7,<br>\n", "                   test_data_8,<br>\n", "                   test_data_9<br>\n", "                  ],sort=False,axis=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["f2 = pd.read_csv(full_file_path)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[7]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_X=df2[X_col_names]\n", "df_y=df2[[y_col_name]]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[8]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X=np.array(df_X)\n", "y=np.array(df_y)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Functions"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[12]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["@title get_df_w_metric(double click to expand)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_df_w_metrics(df,protected_variable_name,y_target_name,y_pred_name):\n", "    \"\"\"\n", "    This function takes a dataframe (df), and returns FPR/FNR for each value in the protected variable\n", "    Input: \n", "        df:                         a dataframe\n", "        protected_variable_name:    the name of the protected variable in the df\n", "        y_target_name:              the name of the target variable in the df\n", "        y_pred_name:                the name of the predticted variable in the df\n", "    \"\"\"\n\n", "    #Create empty DataFrame\n", "    confusion_df=pd.DataFrame(columns=[protected_variable_name,\"FPR\",\"FNR\"])\n", "    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    #For each value of the protected variable, calculated FPR/FNR and insert into the empty DataFrame\n", "    for name in list(df[protected_variable_name].unique()):\n", "        a=df[df[protected_variable_name]==name][y_target_name]\n", "        b=df[df[protected_variable_name]==name][y_pred_name]#.apply(lambda x: 0 if x<t else 1 )\n", "    \n", "        TN, FP, FN, TP = confusion_matrix(list(a), list(b),labels=[0, 1]).ravel()\n", "        \n", "        # Sensitivity, hit rate, recall, or true positive rate\n", "        TPR = TP/(TP+FN)\n", "        # Specificity or true negative rate\n", "        TNR = TN/(TN+FP) \n", "        # Precision or positive predictive value\n", "        PPV = TP/(TP+FP)\n", "        # Negative predictive value\n", "        NPV = TN/(TN+FN)\n", "        # Fall out or false positive rate\n", "        FPR = FP/(FP+TN)\n", "        # False negative rate\n", "        FNR = FN/(TP+FN)\n", "        # False discovery rate\n", "        FDR = FP/(TP+FP)\n\n", "        # Overall accuracy\n", "        ACC = (TP+TN)/(TP+FP+FN+TN)\n", "        LRplus=TPR/FPR\n", "        LRminus=FNR/TNR"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        #F1-score\n", "        F1=2*(PPV*TPR)/(PPV+TPR)\n", "        confusion_df=confusion_df.append({protected_variable_name:name,\n", "                                          \"TPR\":TPR,\n", "                                          \"TNR\":TNR,\n", "                                          \"FPR\":FPR,\n", "                                          \"FNR\":FNR,\n", "                                          \"PPV\":PPV,\n", "                                          \"NPV\":NPV,\n", "                                          \"FDR\":FDR,\n", "                                          \"ACC\":ACC,\n", "                                          \"F1\":F1,\n", "                                          \"LRplus\":LRplus,\n", "                                          \"TN\":TN,\n", "                                          \"FP\":FP,\n", "                                          \"FN\":FN,\n", "                                          \"TP\":TP\n", "                                          },ignore_index=True)\n", "    return confusion_df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[13]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["rom: https://nbviewer.jupyter.org/github/srnghn/bias-mitigation-examples/blob/master/Bias%20Mitigation%20with%20Disparate%20Impact%20Remover.ipynb"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def calc_prop(data, group_col, group, output_col, output_val):\n", "    '''\n", "    data:       The dataframe\n", "    group_col:  The protected atrtibute column (e.g Gender)\n", "    group:      The chosen group (e.g Male or Female)\n", "    output_col: The column holding the y-value (either y_hat or y   - could be Fall)\n", "    output_val: The value of the y  (e.g.   all y=1 )\n", "    \n", "    \n", "    Example:\n", "    \n", "    Find p(y=0 | G=\"Female\")\n", "    \n", "    calc_prop(data,\"Gender\",\"Female\",\"y_true\",0)\n", "    \n", "    \n", "    \n", "    '''\n", "    new = data[data[group_col] == group]\n", "    return len(new[new[output_col] == output_val])/len(new)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[14]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def calc_prop_no_group(data, output_col, output_val):\n", "    return len(data[data[output_col] == output_val])/len(data)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[15]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def compare_bias_metrics(data,protected_variable_name,y_target_name,y_pred_name,unfavourable_name,favourable_name,print_var=False,fav_value=1,unfav_value=0):\n", "    \n", "    \n", "    df = get_df_w_metrics(data,protected_variable_name,y_target_name,y_pred_name)\n", "    \n", "    \n", "    \n", "    #==================== DISPARATE IMPACT ======================#\n", "    #Feldman et al\n", "    \n", "    #a1=data[y_target_name]\n", "    #b1=data[protected_variable_name]#.apply(lambda x: 0 if x<t else 1 )\n", "    #d, b, c, a = confusion_matrix(list(a1), list(b1),labels=[0, 1]).ravel()\n", "    \n", "    a=calc_prop(data,protected_variable_name,unfavourable_name,y_target_name,unfav_value) #prop of unfav group, recieve unfav value\n", "    b=calc_prop(data,protected_variable_name,favourable_name,y_target_name,unfav_value) #prop of ufav group, recieve unfav value\n", "    c=calc_prop(data,protected_variable_name,unfavourable_name,y_target_name,fav_value) #prop of ufav group, recieve fav value\n", "    d=calc_prop(data,protected_variable_name,favourable_name,y_target_name,fav_value) #prop of ufav group, recieve fav value\n", "    \n", "    \n", "    #a=df[df[protected_variable_name]==unfavourable_name][\"TN\"].item()\n", "    #b=df[df[protected_variable_name]==favourable_name][\"TN\"].item()\n", "    #c=df[df[protected_variable_name]==unfavourable_name][\"TP\"].item()\n", "    #d=df[df[protected_variable_name]==favourable_name][\"TP\"].item()\n", "        \n", "    Feldman_Disparate_impact=(c/(a + c)) / (d/(b + d))\n", "    \n", "    if print_var:\n", "        print(\"==================================== Feldman et al ====================================\")\n", "        \n", "        print(f\"If DATASET has no disparate impact then DI>=0.8.\")\n", "        print(f\"DI={Feldman_Disparate_impact}\")\n", "        \n", "        if Feldman_Disparate_impact>=0.8:\n", "            print(\"The DATASET has no disparate impact\")\n", "        else:\n", "            print(\"The DATASET has disparate impact\")\n", "        print(\"======================================================================================= \\n\")\n", "            \n", "    #===========================================================#\n", "    \n", "    \n", "    #==================== Learning Fair representations ======================#\n", "    #Zafar et al\n", "    \n", "    ###Disparate TREATMENT ####\n", "    \n", "    p_yhat1_z1=calc_prop(data,protected_variable_name,favourable_name,y_pred_name,fav_value)\n", "    p_yhat1_z0=calc_prop(data,protected_variable_name,unfavourable_name,y_pred_name,fav_value)\n", "    p_yhat1=calc_prop_no_group(data, y_pred_name, fav_value)\n", "    \n", "    p_yhat0_z1=calc_prop(data,protected_variable_name,favourable_name,y_pred_name,unfav_value)\n", "    p_yhat0_z0=calc_prop(data,protected_variable_name,unfavourable_name,y_pred_name,unfav_value)\n", "    p_yhat0=calc_prop_no_group(data, y_pred_name, unfav_value)\n", "    \n", "    \n", "    ###Disparate IMPACT ####\n", "    \n", "    if print_var:\n", "        print(\"==================================== Zafar et al ====================================\")\n", "        \n", "        print(f\"If the CLASSIFIER has no DISPARATE TREATMENT, these equations should hold:\")\n", "        print(f\"P(y_hat=1|z={favourable_name},x) = P(y_hat=1,x) <=> {round(p_yhat1_z1,2)} = {round(p_yhat1,2)}\")\n", "        print(f\"P(y_hat=1|z={unfavourable_name},x) = P(y_hat=1,x) <=> {round(p_yhat1_z0,2)} = {round(p_yhat1,2)}\")\n", "        print(f\"P(y_hat=0|z={favourable_name},x) = P(y_hat=0,x) <=> {round(p_yhat0_z1,2)} = {round(p_yhat0,2)}\")\n", "        print(f\"P(y_hat=0|z={unfavourable_name},x) = P(y_hat=0,x) <=> {round(p_yhat0_z0,2)} = {round(p_yhat0,2)}\")\n", "        print(\"\\n\")\n", "        \n", "        print(f\"If the CLASSIFIER has no DISPARATE IMPACT, these equations should hold:\")\n", "        print(f\"P(y_hat=1|z={unfavourable_name}) = P(y_hat=1,z={favourable_name}) <=> {round(p_yhat1_z0,2)} = {round(p_yhat1_z1,2)}\")\n", "        print(\"\\n\")\n", "        \n", "        print(f\"If the CLASSIFIER has no DISPARATE MISTREATMENT, these equations should hold:\")\n", "        \n", "        FPR_z0=df[df[protected_variable_name]==unfavourable_name][\"FPR\"].item()\n", "        FPR_z1=df[df[protected_variable_name]==favourable_name][\"FPR\"].item()\n", "        \n", "        FNR_z0=df[df[protected_variable_name]==unfavourable_name][\"FNR\"].item()\n", "        FNR_z1=df[df[protected_variable_name]==favourable_name][\"FNR\"].item()\n", "        \n", "        \n", "        \n", "        print(f\"FPR: P(y_hat!=y|z={unfavourable_name},y=0) = P(y_hat!=y|z={favourable_name},y=0) <=> {round(FPR_z0,2)} = {round(FPR_z1,2)}\")\n", "        print(f\"FNR: P(y_hat!=y|z={unfavourable_name},y=1) = P(y_hat!=y|z={favourable_name},y=1) <=> {round(FNR_z0,2)} = {round(FNR_z1,2)}\")\n", "        \n", "        print(\"======================================================================================= \\n\")\n", "    \n", "    \n", "    \n", "    \n", "    \n", "    #==================== Equality of Opportunity in Supervised Learning ======================#\n", "    #Hardt et al\n", "    \n", "    ### Equalized odds ####\n", "    \n", "    TPR_z0=df[df[protected_variable_name]==unfavourable_name][\"TPR\"].item()\n", "    TPR_z1=df[df[protected_variable_name]==favourable_name][\"TPR\"].item()\n", "    \n", "    TNR_z0=df[df[protected_variable_name]==unfavourable_name][\"TNR\"].item()\n", "    TNR_z1=df[df[protected_variable_name]==favourable_name][\"TNR\"].item()\n", "    \n", "    if print_var:\n", "        print(\"==================================== Hardt et al ====================================\")\n", "        \n", "        print(f\"If the CLASSIFIER has EQUALIZED ODDS, these equations should hold:\")\n", "        print(f\"P(y_hat=1|z={unfavourable_name},y=1) = P(y_hat=1|z={favourable_name},y=1) <=> {round(TPR_z0,2)} = {round(TPR_z1,2)}\")\n", "        print(f\"P(y_hat=0|z={unfavourable_name},y=0) = P(y_hat=0|z={favourable_name},y=0) <=> {round(TNR_z0,2)} = {round(TNR_z1,2)}\")\n", "        print(\"\\n\")\n", "        \n", "        print(f\"If the CLASSIFIER has EQUAL OPPORTUNITY, these equations should hold:\")\n", "        print(f\"P(y_hat=1|z={unfavourable_name},y=1) = P(y_hat=1|z={favourable_name},y=1) <=> {round(TPR_z0,2)} = {round(TPR_z1,2)}\")\n", "        print(\"\\n\")\n", "    \n", "        print(\"======================================================================================= \\n\")\n", "    \n", "    \n", "    ############Measuring racial discrimination in algorithms####\n", "    #Arnold et al. \n", "    \n", "    \n", "    \n", "    if print_var:\n", "        print(\"==================================== Arnold et al ====================================\")\n", "        \n", "        my=data[y_target_name].mean()\n", "        delta=(TNR_z1-TNR_z0)*(1-my)+(FNR_z1-FNR_z0)*my\n", "        \n", "        \n", "        print(f\"The racial discrimination paramenter (delta) = {delta}\")\n", "        print(\"\\n\")\n", "    \n", "        print(\"======================================================================================= \\n\")\n", "    \n", "        \n", "        \n", "        \n", "        \n", "    ############GENERAL CLASSIFICATION METRICS####\n", "    \n", "    \n", "    \n", "    if print_var:\n", "        print(\"==================================== GENERAL CLASSIFICATION METRICS ====================================\")\n", "        \n", "        print(f\"TPR for {unfavourable_name}: {TPR_z0}\")\n", "        print(f\"TPR for {favourable_name}: {TPR_z1}\")\n", "        print(\"\\n\")\n", "        \n", "        print(f\"TNR for {unfavourable_name}: {TNR_z0}\")\n", "        print(f\"TNR for {favourable_name}: {TNR_z1}\")\n", "        print(\"\\n\")\n", "        \n", "        print(f\"FPR for {unfavourable_name}: {FPR_z0}\")\n", "        print(f\"FPR for {favourable_name}:  {FPR_z1}\")\n", "        print(\"\\n\")\n", "        \n", "        print(f\"FNR for {unfavourable_name}:  {FNR_z0}\")\n", "        print(f\"FNR for {favourable_name}:  {FNR_z1}\")\n", "        print(\"\\n\")\n", "    \n", "        print(\"======================================================================================= \\n\")\n", "    \n", "        \n", "        \n", "        \n", "    \n", "    \n", "    \n", "    return Feldman_Disparate_impact\n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": [" "]}, {"cell_type": "markdown", "metadata": {}, "source": [" "]}, {"cell_type": "markdown", "metadata": {}, "source": [" "]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Identify bias"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[25]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f\"The dataset has {df2.shape[0]} rows\")\n", "print(f\"The dataset has {df2.shape[1]} cols\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f\"The dataset to test has the protected attribute: {procted_col_name}\")\n", "print(f\"The protected variable can be assigned: {df2[procted_col_name].unique()}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f\"The ratio of records with y=1: {df2[df2[y_col_name]==1][y_col_name].count()/df2[y_col_name].count()}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[26]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["metrics=compare_bias_metrics(data=df2,\n", "                        protected_variable_name=procted_col_name,\n", "                        y_target_name=y_col_name,\n", "                        y_pred_name=output_col_name,\n", "                        unfavourable_name=0,\n", "                        favourable_name=1,\n", "                        print_var=True\n", "                        \n", "                       )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}